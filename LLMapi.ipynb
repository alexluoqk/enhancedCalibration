{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def get_chat_gpt_response(prompt):\n",
    "    url = \"https://api.kksj.org/v1/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": \"sk-fsxo886oOAOfcPzY1e0aB6F1A74548Ab83D3A9866161Fd5d\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    data = {\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "        \"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": prompt}]\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'chatcmpl-89Dt2y9M7Bbphjer8VfYCs9DMXG5A', 'object': 'chat.completion', 'created': 1720000355, 'model': 'gpt-3.5-turbo-0125', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': \"Hello! I'm just a virtual assistant, so I don't have feelings, but I'm here and ready to help you with whatever you need. How can I assist you today?\"}, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 19, 'completion_tokens': 37, 'total_tokens': 56}}\n"
     ]
    }
   ],
   "source": [
    "response = get_chat_gpt_response(\"Hello, how are you?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当然可以！以下是一个关于太阳和月亮的学术笑话：\n",
      "\n",
      "在一次天文学学术会议上，太阳和月亮也被邀请作为特别嘉宾。会议期间，主持人问太阳：“太阳先生，您每天都在天空中高高挂起，照耀大地，您觉得自己最大的贡献是什么？”\n",
      "\n",
      "太阳骄傲地回答：“当然是提供光和热，让地球上的生命得以生存和繁衍！”\n",
      "\n",
      "主持人点点头，又转向月亮：“那月亮小姐，您呢？您觉得自己最大的贡献是什么？”\n",
      "\n",
      "月亮微微一笑，说：“我最大的贡献就是让那些天文学家们有更多的时间来研究我，因为他们白天都在睡觉！”\n",
      "\n",
      "这个笑话通过拟人化的方式，幽默地反映了天文学家们夜以继日的工作状态，同时也突出了太阳和月亮在天文学研究中的重要性。\n",
      "stream end\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "\n",
    "headers = {\n",
    "    'Authorization': 'sk-fsxo886oOAOfcPzY1e0aB6F1A74548Ab83D3A9866161Fd5d',\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "userContentData = '给我讲一个学术笑话,具体来说是关于太阳和月亮的'\n",
    "\n",
    "modelDataList = ['gpt-4-all', 'gpt-4o', 'gpt-4-turbo', 'gemini-1.5-pro', 'claude-3-opus-20240229', 'gpt-3.5-turbo']\n",
    "\n",
    "data = {\n",
    "    'model': modelDataList[1],\n",
    "    'messages': [\n",
    "        {'role': 'system', 'content': '你是一位中文学术写作专家'},\n",
    "        {'role': 'user', 'content': userContentData}\n",
    "    ],\n",
    "    \"stream\": True,\n",
    "    \"temperature\": 0\n",
    "}\n",
    "\n",
    "with requests.post('https://api.kksj.org/v1/chat/completions', headers=headers, json=data, stream=True) as response:\n",
    "    for line in response.iter_lines():\n",
    "        # Filter out keep-alive new lines\n",
    "        if line:\n",
    "            decoded_line = line.decode('utf-8')\n",
    "\n",
    "            if decoded_line == 'data: [DONE]':\n",
    "                break\n",
    "\n",
    "            if decoded_line.startswith('data: '):\n",
    "                data_str = decoded_line[len('data: '):]\n",
    "                data_json = json.loads(data_str)\n",
    "\n",
    "                # 检查 choices 是否为空\n",
    "                choices = data_json.get('choices', [])\n",
    "                if not choices:\n",
    "                    continue\n",
    "                finish_reason = choices[0].get('finish_reason', '')\n",
    "                if finish_reason == 'stop':\n",
    "                    break\n",
    "\n",
    "                # 检查 delta 是否存在并获取 content\n",
    "                delta = choices[0].get('delta', {})\n",
    "                content = delta.get('content', '')\n",
    "                if content:\n",
    "                    print(f'{content}', end=\"\")\n",
    "    print()\n",
    "\n",
    "print('stream end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m chunk_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time  \u001b[38;5;66;03m# calculate the time delay of the chunk\u001b[39;00m\n\u001b[1;32m     36\u001b[0m collected_chunks\u001b[38;5;241m.\u001b[39mappend(chunk)  \u001b[38;5;66;03m# save the event response\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m chunk_message \u001b[38;5;241m=\u001b[39m \u001b[43mchunk\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mchoices\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdelta\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# extract the message\u001b[39;00m\n\u001b[1;32m     38\u001b[0m collected_messages\u001b[38;5;241m.\u001b[39mappend(chunk_message)  \u001b[38;5;66;03m# save the message\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMessage received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds after request: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# print the delay and text\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers, not 'str'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "headers = {\n",
    "    'Authorization': 'sk-fsxo886oOAOfcPzY1e0aB6F1A74548Ab83D3A9866161Fd5d',\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "userContentData = '给我讲一个学术笑话,具体来说是关于太阳和月亮的'\n",
    "\n",
    "modelDataList = ['gpt-4-all', 'gpt-4o', 'gpt-4-turbo', 'gemini-1.5-pro', 'claude-3-opus-20240229', 'gpt-3.5-turbo']\n",
    "\n",
    "data = {\n",
    "    'model': modelDataList[1],\n",
    "    'messages': [\n",
    "        {'role': 'system', 'content': '你是一位中文学术写作专家'},\n",
    "        {'role': 'user', 'content': userContentData}\n",
    "    ],\n",
    "    \"stream\": True,\n",
    "    \"temperature\": 0\n",
    "}\n",
    "\n",
    "# create variables to collect the stream of chunks\n",
    "collected_chunks = []\n",
    "collected_messages = []\n",
    "\n",
    "with requests.post('https://api.kksj.org/v1/chat/completions', headers=headers, json=data, stream=True) as response:\n",
    "\n",
    "# # iterate through the stream of events\n",
    "#     for line in response.iter_lines():\n",
    "#         chunk = line.decode('utf-8')\n",
    "#         chunk_time = time.time() - start_time  # calculate the time delay of the chunk\n",
    "#         collected_chunks.append(chunk)  # save the event response\n",
    "#         chunk_message = chunk['choices'][0]['delta']  # extract the message\n",
    "#         collected_messages.append(chunk_message)  # save the message\n",
    "#         print(f\"Message received {chunk_time:.2f} seconds after request: {chunk_message}\")  # print the delay and text\n",
    "\n",
    "# print the time delay and text received\n",
    "print(f\"Full response received {chunk_time:.2f} seconds after request\")\n",
    "full_reply_content = ''.join([m.get('content', '') for m in collected_messages])\n",
    "print(f\"Full conversation received: {full_reply_content}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bardapi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
